{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper transcription on the FLEURS dataset\n",
    "Only on the downloaded part of it (approx. 10 languages for which I found a multilingual NeMo model as comparision). The manifests need to be generated beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kozi/Documents/_onlab_git/ami/tinydiarize/whisper/timing.py:57: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    }
   ],
   "source": [
    "# if faster_whisper fails silently, chances are this will solve it: sudo apt install nvidia-cudnn # ipynb does not show all errors properly; I recommend running the problematic scripts directly from the terminal\n",
    "\n",
    "import whisper\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from faster_whisper import WhisperModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a dedicated faster_whisper_model\n",
    "def download_faster_whisper_model(model_path_or_name):\n",
    "    model = WhisperModel(model_path_or_name, device=\"cuda\", compute_type=\"int8\", local_files_only = False)\n",
    "    return model\n",
    "\n",
    "#model = download_faster_whisper_model('bababababooey/faster-whisper-large-v3') # 'large-v3'\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_whisper_model(model_path_or_name):\n",
    "    model = whisper.load_model(model_path_or_name, device=\"cuda\")#.to(torch.device(\"cpu\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_faster_whisper_model(model_path_or_name):\n",
    "    # this can load even large models\n",
    "    if model_path_or_name == 'large-v3':\n",
    "        model = WhisperModel('bababababooey/faster-whisper-large-v3', device=\"cuda\", compute_type=\"int8\", local_files_only = True)\n",
    "        model.feature_extractor.mel_filters = model.feature_extractor.get_mel_filters(model.feature_extractor.sampling_rate, model.feature_extractor.n_fft, n_mels=128)\n",
    "    else:\n",
    "        model = WhisperModel(model_path_or_name, device=\"cuda\", compute_type=\"int8\", local_files_only = True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# faster_whisper is faster than openai's implementation\n",
    "def whisper_transcribe_from_manifest(model, manifest_path, output_path, openai_whisper=False):\n",
    "    results = []\n",
    "\n",
    "    with open(manifest_path, 'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "        print(\"read file {}\".format(manifest_path))\n",
    "        for line in tqdm(lines):\n",
    "            # load() for whole document, loads() for string\n",
    "            manifest_entry = json.loads(line)\n",
    "            #print(manifest_entry['audio_filepath'])\n",
    "\n",
    "            if openai_whisper:\n",
    "                result = model.transcribe(manifest_entry['audio_filepath'])\n",
    "            else:\n",
    "                segments, _ = model.transcribe(manifest_entry['audio_filepath'], beam_size=1, best_of=1) #  beam_size=1, best_of=1\n",
    "                segments = list(segments)  # The transcription will actually run here.)\n",
    "                result = \"\"\n",
    "                for segment in segments:\n",
    "                    result += segment.text\n",
    "                #print(result)\n",
    "\n",
    "            results.append({\n",
    "                \"audio_filepath\": manifest_entry['audio_filepath'],\n",
    "                \"duration\": manifest_entry['duration'],\n",
    "                \"text\": manifest_entry['text'],\n",
    "                \"pred_text\": result\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\") as fout:\n",
    "        for result in results:\n",
    "            # ensure_ascii=True is the default and even speech_transcribe will use escaped umlauts, but this way the manifest is human-readable\n",
    "            fout.write(json.dumps(result, ensure_ascii=False) + \"\\n\")   \n",
    "    print(\"wrote file {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model tiny\n",
      "loaded model base\n",
      "loaded model small\n",
      "loaded model medium\n",
      "loaded model large\n",
      "loaded model large-v2\n",
      "loaded model large-v3\n",
      "read file /home/kozi/Documents/fleurs/hu_hu/whisper_manifest.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a49f936a0742f5a7877b8c15958821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/905 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote file /home/kozi/Documents/fleurs/hu_hu/whisper_transcription_hu_hu_large-v3.json\n"
     ]
    }
   ],
   "source": [
    "# run faster-whisper transcription for the downloaded fleurs dataset (manifests need to be generated beforehand)\n",
    "# fleurs_dataset_dirs = [\"be_by\", \"de_de\", \"en_us\", \"fr_fr\", \"gl_es\", \"hr_hr\", \"hu_hu\", \"it_it\", \"pl_pl\", \"ru_ru\", \"uk_ua\"]\n",
    "\n",
    "# large model does not fit into 4GB of VRAM (even medium can become problematic)\n",
    "# actually it does with cudnn, see ref. https://github.com/guillaumekln/faster-whisper#large-v2-model-on-gpu\n",
    "for model_name in [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]: \n",
    "    model = load_faster_whisper_model(model_name)\n",
    "    print(\"loaded model {}\".format(model_name))\n",
    "\n",
    "    for child_item in os.listdir(\"/home/kozi/Documents/fleurs/\"):\n",
    "        child_dir = os.path.join(\"/home/kozi/Documents/fleurs/\", child_item)\n",
    "        if not os.path.isdir(child_dir):\n",
    "            continue\n",
    "\n",
    "        input_manifest_path = os.path.join(child_dir, \"whisper_manifest.json\")\n",
    "        if not os.path.isfile(input_manifest_path):\n",
    "            continue\n",
    "\n",
    "        output_manifest_path = os.path.join(child_dir, \"whisper_transcription_{}_{}.json\".format(child_item, model_name))\n",
    "        if os.path.isfile(output_manifest_path):\n",
    "            continue\n",
    "\n",
    "        whisper_transcribe_from_manifest(\n",
    "            model,\n",
    "            input_manifest_path,\n",
    "            output_path = output_manifest_path,\n",
    "            openai_whisper = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model tiny\n",
      "read file /home/kozi/Documents/librispeech_get_test_dataset/data/test_other_whisper.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722cfffceda2446a914cd9f0a1b92b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote file /home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_tiny.json\n",
      "loaded model base\n",
      "read file /home/kozi/Documents/librispeech_get_test_dataset/data/test_other_whisper.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d61adbaef674e229883e1d6cb92c317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote file /home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_base.json\n",
      "loaded model small\n",
      "read file /home/kozi/Documents/librispeech_get_test_dataset/data/test_other_whisper.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15662a12af964ccc83e0dfe471e1803d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote file /home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_small.json\n",
      "loaded model medium\n",
      "read file /home/kozi/Documents/librispeech_get_test_dataset/data/test_other_whisper.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587136e8478c4d43b9b448bc664251a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote file /home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_medium.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA failed with error out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run faster-whisper transcription for the downloaded librispeech dataset (manifests need to be generated beforehand)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# fleurs_dataset_dirs = [\"be_by\", \"de_de\", \"en_us\", \"fr_fr\", \"gl_es\", \"hr_hr\", \"hu_hu\", \"it_it\", \"pl_pl\", \"ru_ru\", \"uk_ua\"]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# large model does not fit into 4GB of VRAM (even medium can become problematic)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# actually it does with cudnn, see ref. https://github.com/guillaumekln/faster-whisper#large-v2-model-on-gpu\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiny\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m]: \n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_faster_whisper_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded model \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name))\n\u001b[1;32m     10\u001b[0m     input_manifest_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/kozi/Documents/librispeech_get_test_dataset/data/test_other_whisper.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mload_faster_whisper_model\u001b[0;34m(model_path_or_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mmel_filters \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mget_mel_filters(model\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39msampling_rate, model\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mn_fft, n_mels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path_or_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/nemo2/lib/python3.8/site-packages/faster_whisper/transcribe.py:128\u001b[0m, in \u001b[0;36mWhisperModel.__init__\u001b[0;34m(self, model_size_or_path, device, device_index, compute_type, cpu_threads, num_workers, download_root, local_files_only)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m download_model(\n\u001b[1;32m    123\u001b[0m         model_size_or_path,\n\u001b[1;32m    124\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    125\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mdownload_root,\n\u001b[1;32m    126\u001b[0m     )\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mctranslate2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWhisper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintra_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43minter_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m tokenizer_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(tokenizer_file):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA failed with error out of memory"
     ]
    }
   ],
   "source": [
    "# run faster-whisper transcription for the downloaded librispeech dataset (manifests need to be generated beforehand)\n",
    "# fleurs_dataset_dirs = [\"be_by\", \"de_de\", \"en_us\", \"fr_fr\", \"gl_es\", \"hr_hr\", \"hu_hu\", \"it_it\", \"pl_pl\", \"ru_ru\", \"uk_ua\"]\n",
    "\n",
    "# large model does not fit into 4GB of VRAM (even medium can become problematic)\n",
    "# actually it does with cudnn, see ref. https://github.com/guillaumekln/faster-whisper#large-v2-model-on-gpu\n",
    "for model_name in [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]: \n",
    "    model = load_faster_whisper_model(model_name)\n",
    "    print(\"loaded model {}\".format(model_name))\n",
    "\n",
    "    input_manifest_path = \"/home/kozi/Documents/librispeech_get_test_dataset/data/test_other_whisper.json\"\n",
    "\n",
    "    output_manifest_path = \"/home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_{}.json\".format(model_name)\n",
    "    if os.path.isfile(output_manifest_path):\n",
    "        continue\n",
    "\n",
    "    whisper_transcribe_from_manifest(\n",
    "        model,\n",
    "        input_manifest_path,\n",
    "        output_path = output_manifest_path,\n",
    "        openai_whisper = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to calculate results for librispeech dataset\n",
    "\n",
    "!python3 /home/kozi/Documents/NeMo/examples/asr/speech_to_text_eval.py \\\n",
    "    dataset_manifest='/home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/out_large_CER_1.json' \\\n",
    "    use_cer=True \\\n",
    "    only_score_manifest=True\n",
    "\n",
    "!python3 /home/kozi/Documents/NeMo/examples/asr/speech_to_text_eval.py \\\n",
    "    dataset_manifest='/home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_tiny.json' \\\n",
    "    use_cer=True \\\n",
    "    only_score_manifest=True\n",
    "\n",
    "!python3 /home/kozi/Documents/NeMo/examples/asr/speech_to_text_eval.py \\\n",
    "    dataset_manifest='/home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_base.json' \\\n",
    "    use_cer=True \\\n",
    "    only_score_manifest=True\n",
    "\n",
    "!python3 /home/kozi/Documents/NeMo/examples/asr/speech_to_text_eval.py \\\n",
    "    dataset_manifest='/home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_small.json' \\\n",
    "    use_cer=True \\\n",
    "    only_score_manifest=True\n",
    "\n",
    "!python3 /home/kozi/Documents/NeMo/examples/asr/speech_to_text_eval.py \\\n",
    "    dataset_manifest='/home/kozi/Documents/_onlab_git/output/librispeech_get_test_dataset/whisper_transcription_librispeech_medium.json' \\\n",
    "    use_cer=True \\\n",
    "    only_score_manifest=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
